

A tale of two projects: JGroups and jgroups-raft
================================================
:author: Bela Ban belaban@yahoo.com
:backend: deckjs
:deckjs_transition: fade
:navigation:
:deckjs_theme: web-2.0
:deckjs_transition: fade
:goto:
:menu:
:toc:
:status:


About me
--------
* Bela Ban, Kreuzlingen (Switzerland)
* PhD in Zurich, post-doc at Cornell where I started JGroups in 1998
* I've been working on JGroups ever since
* Joined JBoss in 2003 as employee 007
* JBoss was acquired by RedHat in 2006
* Started JBossCache (predecessor of Infinispan) in 2003, worked on it until version 1.4
* Started jgroups-raft end of 2014
* I enjoy biking, mountain biking, tennis, running
* I hate maven, scala, fat monolithic projects, security


What is JGroups?
----------------
* Library (a single 2MB JAR) for _reliable group communication_:
** One-to-many or one-to-one communication in a cluster
** Cluster management (join, leave, membership management)
* Adds reliability to IP multicasting
** Retransmission, deliver-once, FIFO or total ordering
** TCP also supported as transport



When should I use it?
---------------------
* When your application needs to
** discover the nodes of a cluster automatically
** add or remove nodes _dynamically_
** send messages to all (or a subset of) nodes, or to single nodes _reliably_
** rely on all messages arriving in a given order
** get notified when nodes join or leave (or crash)
** customize QoS for cluster communication



Typical applications
--------------------
* Replicated or distributed state (session state in application servers)
* Communication in a cluster
* Chat
* Task distribution
* Presence
* Pub/sub


Other factoids
--------------
* Runs on Android, too
* Largest cluster: 2'238 nodes on Google Compute Engine
* Very stable: `currentYear()-1998` years old




JGroups has 3 parts
-------------------
. JChannel
* A _cluster node endpoint_. Like a socket

. Protocol stack
* Users can add, remove, replace, enhance, or even write their own protocols
* Ships protocols for
** Network communication (transport)
** Membership discovery
** Failure detection
** Lossless and ordered transmission
** Network split handling and subsequent merging
** Notification when nodes join or leave the cluster (membership)
** Flow control
** Fragmentation
** Compression, encryption, authentication
* Stack can be adapted to every environment

. Building blocks
* Used over a channel
** Method invocation across a cluster
** Distributed caches, counters, locks, task execution



Architecture
------------
* Users deal mainly with the _channel_ (`JChannel`)
* The protocol stack is setup according to an XML config file
* A sent message passes the stack _top-down_
* A received message is passed up through the stack _bottom-up_

image::../images/arch.png[Architecture of JGroups,width="60%",align=left,valign=top]



JChannel
--------
* Create channel
* Join or leave a cluster
* Receive view updates
* Send / receive messages
* Close the channel



Example 1: disseminating stock updates
--------------------------------------

[source,java]
----
protected static void start(String name) throws Exception {
    JChannel ch=new JChannel("/home/bela/fast.xml").name(name);
    ch.setReceiver(new ReceiverAdapter() {
        public void viewAccepted(View view) {
            System.out.printf("-- view: %s\n", view);
        }
        public void receive(Message msg) {
            Update update=(Update)msg.getObject();
            System.out.printf("%s: %.2f\n", update.name, update.val);
        }

    });
    ch.connect("stock-ticker");
    for(;;) {
        String ticker=stocks[((int)Util.random(stocks.length - 1))];
        ch.send(null, new Update(ticker, Util.random(200)));
        Util.sleep(3000);
    }
}

protected static class Update implements Serializable {
    protected String name;
    protected double val;

    public Update(String name, double val) {this.name=name;this.val=val;}
}
----



Building block: RpcDispatcher
-----------------------------
* Building block to invoke _group methods_ (= methods in all cluster nodes)
* Blocking or non-blocking
* Filtering, response collection, timeouts
* get-all, get-first, get-n
* Handles target crashes



Example 2: updating a shared cache
----------------------------------

[source,java]
-----
public V put(K key, V value) {
    V prev_val=get(key);
    try {
        MethodCall call=new MethodCall(PUT, key, value); // calls _put()
        disp.callRemoteMethods(null, call, call_options);
    }
    catch(Exception e) {
        throw new RuntimeException("put(" + key + ", " + value + ") failed", e);
    }
    return prev_val;
}
-----

[source,java]
----
public V _put(K key, V value) {
    V retval=map.put(key, value);
    return retval;
}
----


Protocol stack
--------------
* 80+ protocols
* Default stack has 16 protocols
* Defined via XML (or programmatically):

[source,xml]
----
<config>
    <UDP mcast_port="${jgroups.udp.mcast_port:45588}" />
    <PING />
    <MERGE3 max_interval="30000"
            min_interval="10000"/>
    <FD_SOCK/>
    <FD_ALL/>
    <pbcast.NAKACK2 xmit_interval="500"/>
    <UNICAST3 xmit_interval="500"/>
    <pbcast.STABLE desired_avg_gossip="50000"
                   max_bytes="4M"/>
    <pbcast.GMS print_local_addr="true" join_timeout="2000"
                view_bundling="true"/>
    <UFC max_credits="2M" min_threshold="0.4"/>
    <MFC max_credits="2M" min_threshold="0.4"/>
    <FRAG2 frag_size="60K"  />
</config>
----


Properties of sample stack
--------------------------
* UDP: transport, sends and receives messages. Performs serialization.
* PING: initial discovery to find members
** ~15 discovery protocols, e.g. multicasting, static list, DB, lookup server, AWS, Google CE, OpenStack, Rackspace etc
* MERGE3: detects network partitions (split brain) and merges the subgroups back into one
* FD_SOCK / FD_ALL: failure detection, detects crashes and asks for removal of crashed members
* NAKACK2 / UNICAST3: reliable, lossless, exactly-once, ordered message delivery
* GMS: group membership, handles joins / leaves / crashes, notifies members
* UFC / MFC: flow control
* FRAG2: fragmentation
* More prots: encryption and authentication, compression, stats, distr locks and counters etc


Advantages of protocol stacks
-----------------------------
* Custom configuration matching requirements and (network) environment
* Remove / add protocols
* Write custom protocols (e.g. extending existing ones)
* Divide-and-conquer: each prot does one thing only, and does it well



Demo
----


RAFT consensus in JGroups: jgroups-raft
---------------------------------------
* Experiment with consensus in the face of network partitions
* CAP: pick Availability or Consistency (Partitions always need to be handled)
* AP: consistency can be _eventual_ instead of strict; temporary inconsistency is ok, system always has to be available
** Amazon (Dynamo), Infinispan with EC config
* CP: system has to be consistent at all times, but may not be available when consistency cannot be guaranteed
** jgroups-raft, Infinispan
* JGroups-internal services might benefit, too: distributed counter and locks, sequencer-based total order protocol
** These services all use a leader to acquire a lock, increment a counter or assign a global sequence number
** In a split-brain scenario, we can have multiple leaders
** A _merge_ would fix this, but the harm (inconsistency) is already done


What
----
* RAFT is a consensus based algorithm for replicated state machines [1]
** Consensus used for leader election and committing of changes
* Fixed size cluster (e.g. of 5): `{A,B,C,D,E}`
* _Leaders_ and _followers_; leaders are elected by majority votes (3) (and highest commit log)
** There's always only *a single leader* in a cluster
** All requests are handled by the leader
** Minority partitions (e.g. 2 or less) become unavailable; client requests are rejected
* Leader `A` receives a client request and appends it to its persistent log
* `A` then sends a message to followers `B`, `C`, `D` and `E` which append the change to their logs
* When A receives an ack from 2 other members (majority == 3), it _commits_ the change and applies it to its
  attached _state machine_
* `A` periodically sends its highest commit ID (also acting as a heartbeat) to all followers, who then commit as well
* When the followers update their commit index, they also apply the changes to their state machines
* Similar to `SEQUENCER` (total order), except that `SEQUENCER` can have multiple coordinators in case of network partitions


Difference to 2PC
-----------------
* In 2PC, _all_ TX participants have to ack the prepare
* In RAFT, only a majority needs to ack
** Slow members don't slow everybody else down
* Members which are completely out of sync, or new joiners without a log get sync'ed via snapshot installation
** Similar to state transfer


How
---
* Implementation of the RAFT consensus algorithm [1] in JGroups
* Separate project *jgroups_raft* [2], might get merged into JGroups at some point
* New protocols `ELECTION`, `RAFT` and `CLIENT` (client redirection to the current leader)
* `StateMachine` interface, demo implementation `ReplicatedStateMachine` and demo `ReplicatedStateMachineDemo`
* Persistent `Log` interface with impls based on MapDB (JDBM4) and LevelDB (default) (Ugo)
* Demo

Status
------
* Version 0.2 (alpha quality)
* Includes log appending, log compaction and snapshotting, dynamic cluster topology changes
* Can be used to experiment with RAFT consensus


Todo list
---------
* Use JGroups' failure detection and view installation to replace part of the current code (less code)
* Full unit test coverage



Conclusion
----------
* JGroups is a small-footprint library to write clustered applications
* Very simple API with 6 major methods
* Heavily used in the Java clustering world
* jgroups-raft is relatively new project (based on the mature JGroups project) to experiment with consensus
** See how consensus could be used in a datagrid/caching environment (Infinispan)



Questions?
----------
* JGroups: http://www.jgroups.org
* Infinispan: www.infinispan.org
* [1] RAFT: http://raftconsensus.github.io/
* [2] jgroups-raft: https://github.com/belaban/jgroups-raft
* [3] https://github.com/belaban/jgroups-raft/tree/master/doc
